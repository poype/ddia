# Partitioning 

每个分区都可以视为一个完整的小型数据库。

分区的主要目标是将数据和查询负载均匀分布在所有节点上。

分区通常与复制结合使用，即每个分区在多个节点上都存有副本。这意味着某条记录属于特定的分区，而同样内容会保存在不同的节点上以提高系统的容错性。

一个节点上可能存储了多个分区。

Combining replication and partitioning: each node acts as leader for some partitions and follower for other partitions.

![image-20241020182416805](.\image\image-20241020182416805.png)

上图描述的是主从复制模型与分区组合使用时的数据的分布情况。

如果分区不均匀，则会出现某些分区节点比其它分区承担更多的数据量或查询负载，这称之为**倾斜**。

### 基于关键字区间的分区

![image-20241020183908213](.\image\image-20241020183908213.png)

采用这种分区策略的系统包括**Bigtable**、**HBase**、2.4版本之前的**MongoDB**。

每个分区可以按照关键字排序保存，这样就**可以轻松支持区间查询**。

这种分区策略的缺点是很容易造成**热点**，大量的读写操作都集中在一个分区。

### 基于关键字哈希值的分区

用于数据分区目的的哈希函数不需要在加密方面很强。**Cassandra** 和 **MongoDB** 使用**MD5**作为分区的哈希函数。

关键字根据其哈希值的范围划分到不同的分区中。

![image-20241020184829080](.\image\image-20241020184829080.png)

通过关键字的哈希值进行分区，我们丧失了良好的区间查询特性。即使关键字相邻，经过哈希之后也会分散在不同的分区中。

在MongoDB中，如果启用了哈希分区模式，则区间查询会发送到**所有**的分区上。

很多采用哈希分区策略的数据库干脆就不支持关键字上的区间查询。

但**Cassandra**在区间查询上有一个优秀的设计。

Cassandra中的表的主键可以由多个列组成复合主键。复合主键只有第一部分用于哈希分区，而主键中的其它列则用作组合索引来对Cassandra SSTable中的数据进行排序。因此，Cassandra不支持在第一列上进行区间查询，但如果为第一列指定好了固定值，可在对其它列执行高效的区间查询。

For example, on a social media site, one user may post many updates. If the primary key for updates is chosen to be (user_id, update_timestamp), then you can efficiently retrieve all updates made by a particular user within some time interval, sorted by timestamp. Different users may be stored on different partitions, but within each user, the updates are stored ordered by timestamp on a **single** partition. 

基于哈希的分区方法可以减轻热点，但无法做到完全避免。

## 分区与二级索引

二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询。例如查找所有颜色为红色的汽车。

### 本地索引

Partitioning secondary indexes by document:

![image-20241021085333408](.\image\image-20241021085333408.png)

在这种索引方法中，每个分区完全独立，**各自维护自己的二级索引**，而且只负责自己分区内的文档，不关心其它分区中的数据。

因此文档分区索引也被称为**本地**索引，而不是全局索引。

如果要搜索红色的汽车，就需要将查询发送到**所有**的分区，然后合并**所有**返回的结果。这种查询方法被称为`scatter/gather(分散/聚集)`。这种二级索引的查询代价非常高昂。

**MongoDB、Cassandra、Elasticsearch**支持的二级索引都是这种类型的。

### 全局索引

对所有的数据构建全局索引，而不是每个分区维护自己的本地索引。全局索引页必须进行分区。

![image-20241021220842440](.\image\image-20241021220842440.png)

所有颜色为红色汽车的pk都被收录到`color:red`中，而且这个二级索引本身也是分区的，例如从a到r开始的颜色放在分区0中，从s到z的颜色放在分区1中。

这样的二级索引不需要采用`scatter/gather`对所有分区都执行一遍查询，客户端只需要向对应的分区发送查询请求，所以读取更加高效。

但其写入速度较慢且非常复杂，这是因为单个文档的更新可能会涉及到多个二级索引，而二级索引的分区又可能完全不同被分布在不同的节点上，因此会引入显著的写放大。

对于这种全局二级索引，需要一个跨多个相关分区的分布式事务支持。现有的数据库都不支持**同步**更新二级索引。

目前，对全局二级索引的更新都是**异步**的。如果在写入之后马上根据二级索引去查询，那么刚刚发生的更新可能还没反映到二级索引中。

**Amazon DynamoDB**采用这种二级索引。

## 分区再平衡

数据和请求可以从一个节点转移到另一个节点。这样一个迁移负载的过程称之为再平衡。

1. 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。
2. 再平衡过程中，数据库应该可以继续正常提供读写服务。

即使采用哈希分区策略，也是根据哈希值划分为不同的**区间范围**，然后将每个区间分配给一个分区。

例如，哈希值在区间[0, b0)对应分区0，哈希值在区间[b0, b1)对应分区1等。

不能用取模mod运算进行分区。例如 `hash(key) mod 10` **不能用**！！！这样如果节点数N发生了变化，会导致很多关键字需要从现有的节点迁移到另一个节点，这种迁移操作大大增加了再平衡的成本。

### 固定数量的分区

创建远超实际节点数的分区数，然后为每个节点分配多个分区。

接下来，如果集群中添加了一个新的节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。

Adding a new node to a database cluster with multiple partitions per node:

![image-20241022072735997](.\image\image-20241022072735997.png)

分区再节点之间迁移，但分区的总数量保持不变，也不会改变关键字到分区的映射关系。要调整的**只有**分区与节点之间的对应关系。

**Elasticsearch**采用这种方式进行分区再平衡。

这种策略要求分区的数量在数据库创建时就确定好，之后不能再变。这就要求在初始化时，要充分考虑将来扩容增长的需求，设置一个足够大的分区数。

而每个分区会有额外的管理开销，选择过高的分区数可能会有副作用。

如果分区里的数据量非常大，则每次再平衡或节点故障恢复的代价就很大。但如果一个分区太小，就会产生很多额外的管理开销。

所以分区大小应该“恰到好处”，不要太大，也不能过小。最好能先确定总数据量的未来高度。

### 动态分区

当一个分区的数据量增长超过某个阈值（HBase默认是10GB），这个分区就会被拆分成两个分区，每个新的分区承担一半的数据量。相反，如果大量数据被删除，分区中的数据量缩小到某个阈值以下，则将其与相邻分区进行合并。

每个分区只能分配给一个节点，而每个节点可以承载多个分区。这点与“固定数量的分区”一样。

当一个大的分区发生分裂后，可以将其中的一半转移到其它某个节点以平衡负载。对于HBase，分区文件的传输是借助HDFS。

动态分区的优点是分区数量可以自动适配数据总量。

动态分区不仅适用于关键字区间分区，也适用于哈希的分区策略。

MongoDB 和 HBase 采用动态分区策略实现再平衡。

### 按节点比例分区

“固定数量的分区” 和 “动态分区” 这两种策略，分区的数量与节点的数量**没有**直接关系。

“按节点比例分区” 这个策略的思想是让分区数与集群中的节点数成正比。每个节点具有固定数量的分区。

**Cassandra**就是采用这种方式。

当节点数增加时，分区的数量就会增多，每个分区的数据量就会变小。

当一个新的节点加入集群时，它随机选择一些现有分区进行分裂，拿走那些分区的部分数据量。

Cassandra默认情况下，每个节点有**256**个分区。新节点最终会从现有节点中拿走相当数量的负载。

### 再平衡的代价

再平衡是个比较昂贵的操作，它需要重新路由请求并将大量的数据从一个节点迁移到另一个节点。

## 请求路由

现在数据被分布到多个节点上，当客户端需要发送请求时，如何知道应该连接哪个节点？ 

如果发生了分区再平衡，分区与节点的对应关系还会发生变化。

这其实属于一类典型的服务发现问题。

1. 允许客户端连接任意节点，如果那个节点恰好拥有所请求的分区，就直接处理该请求。否则，将请求转发给目标分区所在的节点，接收答复，并将答复返回给客户端。
2. 将所有客户端请求都发送到一个路由层，由路由层将请求转发到目标分区所在的节点。路由层不直接处理请求，它只是一个分区感知的负载均衡器。
3. 客户端自己感知分区和节点之间的对应关系。客户端直接连接目标节点。

Three different ways of routing a request to the right node:

![image-20241022091936217](.\image\image-20241022091936217.png)

### 分区与节点对应关系跟踪

**HBase、Kafka** 利用 Zookeeper 跟踪分区分配情况。

Using ZooKeeper to keep track of assignment of partitions to nodes:

![image-20241022092221912](.\image\image-20241022092221912.png)

每个节点都向Zookeeper注册自己，Zookeeper维护了分区到节点间的映射关系。路由层（或分区感知的客户端）可以向Zookeeper订阅此信息，一旦分区发生了改变，或者增加、删除节点，Zookeeper就会主动通知路由层，这样使路由信息保持最新状态。

**MongoDB**也是采用类似的设计，但它没有依赖Zookeeper，而是采用自己的配置服务器，mongos守护进程来充当路由层。



**Cassandra**则是采用不同的方法，它在节点之间使用 **gossip** 协议来同步集群状态的变化。请求可以发送给任何节点，由该节点负责将请求转发到目标分区节点。对应下图中对应的方法：

<img src=".\image\image-20241022223947967.png" alt="image-20241022223947967" style="zoom:50%;" />

这种方法增加了数据库节点的复杂性，但是避免了对ZooKeeper的依赖。