### 采用Scale Up扩容的缺点

成本增长过快甚至超过了线性：即如果把一台机器内的CPU数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终成本增加不止一倍(让这些硬件协同工作的成本变高了？)。

由于性能瓶颈因素，一台机器尽管拥有了两倍的硬件指标，但却不一定能处理两倍的负载。

局限于某个特定的地理位置，无法提供异地容错能力。

# 数据复制

数据复制是指通过网络在多台机器上保存相同的数据副本。通过数据复制，可以解决如下问题：

1. 扩展性。多台机器同时提供数据访问服务，从而提高**读**吞吐量。
2. 高可用。多台机器提供冗余，如果某些组件失效，冗余组件可以迅速接管，系统依然可以继续工作。
3. 降低延迟。使数据在地理位置上更接近客户。
   如果客户遍布世界各地，需要在全球范围内部署服务，用户就近访问最近数据中心提供的服务，避免数据请求跨越半个地球。

数据复制方式包括**主从复制、多主复制、无主复制**三种。

主从复制非常流行，主要是因为它很容易理解，也不需要担心冲突问题。

但如果出现节点失效、网络中断或延迟抖动等情况，多主复制和无主复制方案会更加可靠。

## 主从复制

所有客户端的写操作都必须发送给一个节点（主节点），由该节点负责将数据更改事件发送到其它从节点。

每个从节点都可以接收读请求，但从节点可能会返回过期的数据。

再次强调，只有主节点才可以接收写请求，从节点都是只读的。

![image-20241018090532311](.\image\image-20241018090532311.png)

MySQL 和 MongoDB 都支持主从复制。此外Kafka也支持主从复制。

### 同步复制与异步复制

Leader-based replication with one **synchronous** and one **asynchronous** follower:

![image-20241018090956170](.\image\image-20241018090956170.png)

通常情况下，复制速度会非常快，大多数数据库可以在一秒内完成所有从节点的数据同步。

但有些情况下，从节点可能落后主节点几分钟甚至更长时间。例如节点之间的网络出现问题。

同步复制的优点是可以确保主从节点间的一致性，缺点是如果有一个从节点无法及时响应，写操作就不能视为成功。这样会阻塞写操作，而且节点越多，发生故障的概率就越高，所以完全同步的配置现实中反而非常不可靠。

把所有从节点都配置成同步复制有些不切实际。因为这样的话，任何一个节点中断都会导致整个系统写操作被阻塞。

实践中，在系统集群中，**只有一个**从节点采用同步复制，其它从节点都会采用异步方式复制数据。如果同步的从节点不可用，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点（一主一从）拥有最新的数据副本，如果主节点发生宕机，同步的从节点可以立即接管主节点的职责成为新的主节点，这样可以确保数据不会丢失。

当采用全异步模式时，如果主节点宕机，则所有尚未copy到从节点的写操作就会全部丢失。但全异步模式的系统吞吐性能会更好。

### 加入新的从节点

增加一个新的从节点到系统集群中，如何确保新的从节点与主节点保持数据一致？

操作步骤如下：

1. 在某个时间点对主节点的数据副本产生一个快照。MySQL可以利用**innobackupex**创建快照。
2. 将此快照copy到新加入集群的从节点。
3. 从节点连接到主节点请求快照点之后发生的写操作日志。
   在第一步创建快照时，快照会关联到系统复制日志的一个确定位置。这个位置在不同数据库中有不同的称呼，在MySQL中叫做“**binlog coordinates**”。
4. 获得日志之后，从节点应用这些快照点之后的所有数据变更，这个过程称之为追赶。

主节点和从节点上的写入都遵从相同的顺序，而每个节点都维护了复制日志执行的当前偏移量。通过对比主节点和从节点当前偏移量的差值，即可衡量该从节点落后于主节点的程度。

### 节点宕机

个别节点出现宕机，要保持系统总体的持续运行，并尽可能减小节点中断带来的影响。

#### 从节点宕机

根据复制日志偏移量，从节点可以知道在发生故障之前所处理的最后一笔事务。 

从节点恢复之后，它会重新连接到主节点，并请求自那笔事务之后中断期间内所有的数据变更。

#### 主节点宕机

1. 确认主节点失效。
   大多数系统都是采用“超时”机制来确定节点是否存活。节点间频繁地互相发送心跳存活消息，如果发现某个节点在一段比较长的时间内（例如30s）没有响应，即认为那个节点失效了。
2. 选举新的主节点。候选节点最好与原主节点的数据差异最小。
3. 重新配置系统使新的主节点生效：
   a. 客户端需要将写请求发送给新的主节点。
   b. 其它从节点要接受来自新的主节点上的数据变更日志。
   c. 系统要确保原主节点降级为从节点。

目前，即使系统可能支持自动故障切换，有些运维团队仍然愿意以手动方式来控制整个切换过程。

如何设置超时时间、如何确保旧的主节点真的下线了（避免脑裂），这些问题都没有太简单的解决方案。所以手动切换更安全。

### 复制日志的实现

#### 基于SQL语句复制(弃用)

主节点发送INSERT、UPDATE、DELETE语句给从节点。

语句可能存在不确定性，例如 NOW()、RAND()等。

#### 基于预写日志（WAL）传输（不好）

无论是对于日志结构的存储引擎（SSTables和LSM-tree），还是对于覆盖写磁盘的Btree存储引擎，所有对数据库的写操作都会记录日志。可以使用完全相同的日志在另一个节点上构建副本。除了将日志写入磁盘外，主节点还可以通过网络将其发送给从节点。

缺点是WAL日志非常底层，这使得复制方案和存储引擎紧密耦合。如果主从节点的数据库存储格式不同（例如软件版本不同），那么就无法完成复制。这会给后续维护带来很多麻烦。

#### 基于行的逻辑日志复制（MySQL binlog）

复制和存储引擎采用不同的日志格式，这种日志成为逻辑日志。

逻辑日志的内容：

- 对于insert，日志包含一行记录中所有列的值。
- 对于delete，日志中需要包含足够的信息来唯一标识已删除的行，可以靠primary key，但如果表上没有pk，就需要记录所有列的值。
- 对于update，日志中需要包含足够的信息来唯一标识已更新的行，以及已更新列的新值。
- 如果一个事务涉及多行的修改，则会产生多个这样的日志记录，并在后面跟着一条记录，指出该事务已经提交。

MySQL的binlog就是采用的这种方式。

**对于外部应用程序来说，逻辑日志格式也更容易解析**。可以利用binlog将数据库的内容发送到外部系统，例如通过binlog将数据发送给ES、数仓、redis等。

#### 基于触发器的复制（还款管理重构）

通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施自定义的应用层逻辑，例如将数据更改复制到另一个系统。

## 复制滞后造成的问题

采用异步方式传输数据的从节点，数据相比于主节点或其它从节点可能会落后一段时间，这导致应用可能会读到过期的数据。

### 单调读

当读取数据时，单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况。

单调读是一个比强一致性弱，但比最终一致性强的保证。

单调读的一种解决方式是，确保每个用户总是从固定的同一副本读取数据。例如，基于用户ID哈希的方法选择节点副本读取数据，而不是随机选择。

### 前缀一致读

对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序。

## 多主复制

系统存在多个主节点，每个主节点都可以接收写请求。

客户端可以将写请求发送给其中一个主节点，由该主节点负责将数据更改事件转发给其它所有的主节点。每个主节点还同时扮演着其它主节点的从节点。

**在一个数据中心内部使用多主节点没有太大意义**，其复杂性已经超过了它所能带来的好处。

在多数据中心场景下，可以在每个数据中心都配置一个主节点：

![image-20241019191426422](.\image\image-20241019191426422.png)

- Within each datacenter, regular leader–follower replication is used; 
- between datacenters, each datacenter’s leader replicates its changes to the **leaders** in other datacenters.

多主复制的优势：

- 写操作可以发送给离自己最近的数据中心，然后通过异步复制方式将数据变化发送给其它数据中心。这样对上层应用有效屏蔽了数据中心之间的网络延迟，使系统性能更好。
- 数据中心之间通信通常经由广域网，通信质量往往不如数据中心内的本地网络可靠。
  在多主节点架构中，一个主节点会将自己本地的数据写操作通过异步的方式发送给其它数据中心的主节点。这样在遇到网络闪断时不会妨碍写请求最终成功。

MySQL需要借助 Tungsten Replicator 工具实现多主复制。

### 处理写冲突

A write conflict caused by two leaders concurrently updating the same record:

![image-20241019193012955](.\image\image-20241019193012955.png)

#### 1. 避免冲突

处理冲突最理想的方法就是避免冲突的发生，如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突了。

#### 2. 最后写入者获胜 LWW

需要给每个写操作分配一个时间戳。但这种方式容易造成数据丢失。

### 拓扑结构

复制的拓扑结构描述了写操作从一个节点传播到其它节点的通信路径。如果存在**两个以上的主节点**，就会有多个可能的传播拓扑结构。

Three example topologies in which multi-leader replication can be set up:

![image-20241019221907122](.\image\image-20241019221907122.png)

为了防止无限循环，需要给每个节点赋予一个唯一的标识符。在复制日志中的每个写请求都标记了自己已通过的节点标识符。

如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，可以忽略此请求以避免重复转发。

## 无主复制

客户端将写请求同时发送到多个节点上，读取时也要同时从多个节点上并行读取，以此检测和纠正某些过期数据。

Cassandra就是这种风格的数据库。

![image-20241019224405774](.\image\image-20241019224405774.png)

用户1234将写操作并行发送给三个副本，有两个可用副本接受写请求，一个不可用副本无法处理该写请求。用户1234在收到两个成功确认回复之后，即可认为写入操作成功。

在这种情况下，客户1234可以忽略其中一个副本无法执行写操作的情况。

副本3在过了一段时间之后又可以恢复上线，此时副本3上存储的就是过期的旧数据。

当客户2345读取数据时，它不能只向一个副本发送请求，而是**并行地向所有副本**发送读取数据的请求。客户2345可能会得到不同节点的不同响应，包括某些节点上的新值和某些节点上的旧值，通过版本号技术就能确定哪个值更新。

### 读写quorum

如果有n个副本，写操作需要w个节点确认成功，读取必须至少查询r个节点，则只要`w + r > n`，读取的节点中就一定会包含最新值。

满足要求的那些r、w值被称为**法定票数（ quorum ）**。 

一个常见的配置是设置n为某个奇数（例如3或5），w = r = (n + 1) / 2 （向上舍入）。

也可以根据自己的需求灵活调整这些配置。例如，对于读多写少的负载，设置w=n和r=1比较合适，这样读取速度更快。但这样有一个失效的节点就会导致数据库所有的写操作失败。

通常，读取和写入操作总是并行发送到**所有**的n个副本。参数w和参数r只是决定要等待的节点数。即有多少个节点需要返回结果，我们才能判断出结果的正确性。

如果可用的节点数小于所需的w或r，则写入或读取操作就会返回错误。这可能是节点宕机，网络中断，磁盘已满等等原因导致的。

If `w + r > n`, at least one of the r replicas you read from must have seen the most recent successful write：

![image-20241020121404280](.\image\image-20241020121404280.png)

因为成功写入的节点集合和读取的节点集合必然有重复，这样读取的节点中至少有一个具有最新值。

配置适当quorum的数据库可以容忍某些节点故障，也不需要执行故障切换。它们还可以容忍某些节点变慢，这是因为请求并不需要等待所有n个节点的响应，只需要w或r个节点能够响应即可。

### 检测并发写

 Concurrent writes in a Dynamo-style datastore: there is no well-defined ordering.

![image-20241020144602654](.\image\image-20241020144602654.png)

如果节点每当收到新的写请求就简单地覆盖原有的值，那么这些节点将永远无法达成一致。例如在上图中，节点2认为X的最终值是B，而其它节点认为X的值是A。

可以给每个写操作附加一个时间戳，保留时间戳最大的value，丢弃较早的value。这个冲突解决算法被称为最后写入者获胜（last write wins， LWW），这是Cassandra仅有的冲突解决办法。